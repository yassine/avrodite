{# @pebvariable name="context" type="org.avrodite.avro.result.TemplateContext"#}
{# @pebvariable name="test" type="org.avrodite.avro.result.TestRepresentation"#}
## Benchmark Results

#### Test description

The benchmark attempts to capture the end-to-end latency for serialization/deserialization,
like so (in pseudo-code):

```
benchmark() {
  serialize(deserialize(fixture))
}
```

We run {{ context.tests.size() }} tests:
{% for xtest in context.tests %}
{% set test = xtest %}
{{ loop.index + 1 }}. Test {{ loop.index + 1 }} ({{ test.id }}): {{ test.description }}

{% endfor %}

The tests classes are available under the 'avrodite-codec-avro-benchmarks' module.

#### Test Environment

| Component | Description                              |
|-----------|------------------------------------------|
| Processor | Intel(R) Core(TM) i7-6900K CPU @ 3.20GHz |
| Linux     | 5.3.0-29-generic                         |
| JDK       | {{ context.model.vmName }} {{ context.model.vmVersion }}       |
| JVM Args  | {{ context.model.jvmArgsList() }}   |


##### Tested Frameworks

| Framework              | Version    |
|------------------------|------------|
| Avro Core              | 1.8.1      |
| Avrodite               | 0.1.0      |
| Jackson Avro           | 2.8.5      |
| Jackson JSON           | 2.8.5      |
| Protocol Buffers       | 3.11.3     |

#### Results Summary

##### Throughput
{% set test = context.tests[0] %}
| Framework | {% for xtest in context.tests %}{% set test = xtest %}{{ test.throughputMetric.title }} | {{ test.id  }} relative perf. |{% endfor %}

|-----------|{% for xtest in context.tests %}{% set test = xtest %}------------|--------------|{% endfor %}

{% set frameworks = test.throughputMetric.frameworks() %}
{% for framework in frameworks %}
| {{ framework }} | {% for xtest in context.tests %}{% set test = xtest %}{{ test.throughputMetric.metricOfFramework(framework)  }} | {{ test.throughputMetric.relativePerformanceMap[framework]  }} | {% endfor %}

{% endfor %}


##### Heap Usage

{% set test = context.tests[0] %}
| Framework | {% for xtest in context.tests %}{% set test = xtest %}{{ test.heapAllocationRate.title }} | {{ test.id  }} relative perf. |{% endfor %}

|-----------|{% for xtest in context.tests %}{% set test = xtest %}------------|--------------|{% endfor %}

{% set frameworks = test.heapAllocationRate.frameworks() %}
{% for framework in frameworks %}
| {{ framework }} | {% for xtest in context.tests %}{% set test = xtest %}{{ test.heapAllocationRate.metricOfFramework(framework)  }} | {{ test.heapAllocationRate.relativePerformanceMap[framework]  }} | {% endfor %}

{% endfor %}

#### Results Charts
{% for xtest in context.tests %}
{% set test = xtest %}

### {{ test.throughputMetric.title }}

(Higher is better)

![Alt text](./images/{{ test.throughputMetric.imageName() }}?raw=true "{{ test.throughputMetric.title }}")

### {{ test.heapAllocationRate.title }}

(Lower is better)

![Alt text](./images/{{ test.heapAllocationRate.imageName() }}?raw=true "{{ test.heapAllocationRate.title }}")

{% endfor %}



##### Test Fixture (in JSON)

```json
{{ fixtureJson }}

```

#### Use Case 1 : Nullable Fields Schema

```json
{{ t1Schema }}

```

#### Use Case 2 : Non Nullable Fields Schema

```
{{ t2Schema }}

```
